{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable annoying warnings from gensim\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nontrivial polish language grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NONTERMINALS = {\n",
    "    'S': [\n",
    "        ('VERB_PHRASE',),\n",
    "    ],\n",
    "    'VERB_PHRASE': [\n",
    "        ('VERB_IMPS',),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_SG_M1'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_PL_F'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_PL_N1'),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS', 'ACC_PHRASE_SG_M1'),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS', 'ACC_PHRASE_PL_F'),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS', 'ACC_PHRASE_PL_N2'),\n",
    "        \n",
    "    ],\n",
    "    'ADV_PHRASE': [\n",
    "        ('ADV',),\n",
    "        ('ADV', 'ADV',),\n",
    "    ],\n",
    "    'ACC_PHRASE_SG_M1': [\n",
    "        ('SUBST_SG_ACC_M1',),\n",
    "        ('ADJ_PHRASE_SG_ACC_M1', 'SUBST_SG_ACC_M1',),\n",
    "    ],\n",
    "    'ACC_PHRASE_PL_F': [\n",
    "        ('SUBST_PL_ACC_F',),\n",
    "        ('ADJ_PHRASE_PL_ACC_F_N2', 'SUBST_PL_ACC_F'),\n",
    "    ],\n",
    "    'ACC_PHRASE_PL_N2': [\n",
    "        ('SUBST_PL_ACC_N2',),\n",
    "        ('ADJ_PHRASE_PL_ACC_F_N2', 'SUBST_PL_ACC_N2'),\n",
    "    ],\n",
    "    'ADJ_PHRASE_SG_ACC_M1': [\n",
    "        ('ADJ_SG_ACC_M1',),\n",
    "        ('ADJ_SG_ACC_M1', 'ADJ_SG_ACC_M1'),\n",
    "    ],\n",
    "    'ADJ_PHRASE_PL_ACC_F_N2': [\n",
    "        ('ADJ_PL_ACC_F_N2',),\n",
    "        ('ADJ_PL_ACC_F_N2', 'ADJ_PL_ACC_F_N2'),\n",
    "    ],\n",
    "    \n",
    "    # Productions with terminals\n",
    "    'VERB_IMPS': [\n",
    "        ('verb:imps',),\n",
    "    ],\n",
    "    'SUBST_SG_ACC_M1': [\n",
    "        ('subst:sg:acc:m1',),\n",
    "    ],\n",
    "    'SUBST_PL_ACC_F': [\n",
    "        ('subst:pl:acc:f',),\n",
    "    ],\n",
    "    'SUBST_PL_ACC_N2': [\n",
    "        ('subst:pl:acc:n2',),\n",
    "    ],\n",
    "    'ADJ_SG_ACC_M1': [\n",
    "        ('adj:sg:acc:m1',),\n",
    "    ],\n",
    "    'ADJ_PL_ACC_F_N2': [\n",
    "        ('adj:pl:acc:m2.m3.f.n1.n2.p2.p3',),\n",
    "    ],\n",
    "    'ADV': [\n",
    "        ('adv:',),\n",
    "    ]\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINALS = (\n",
    "    'verb:imps',\n",
    "    'subst:sg:acc:m1',\n",
    "    'subst:pl:acc:f',\n",
    "    'subst:pl:acc:n2',\n",
    "    'adj:sg:acc:m1',\n",
    "    'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
    "    'adv:',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoNonterminal(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TooLongSent(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symbol:\n",
    "    def __init__(self, symbol: str):\n",
    "        self.symbol = symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Terminal(Symbol):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nonterminal(Symbol):\n",
    "    def __init__(self, symbol: str, productions: List[Symbol]):\n",
    "        super().__init__(symbol)\n",
    "        self.productions = productions\n",
    "        \n",
    "    def production(self) -> Tuple[Symbol]:\n",
    "\n",
    "        def create_new_symbol(symbol) -> Symbol:\n",
    "           if symbol in NONTERMINALS:\n",
    "               return Nonterminal(symbol, NONTERMINALS[symbol])\n",
    "           else:\n",
    "               return Terminal(symbol)\n",
    "        \n",
    "        # Draw the production\n",
    "        rand_prod_ind = np.random.choice(len(self.productions))\n",
    "        rand_prod = self.productions[rand_prod_ind]\n",
    "            \n",
    "        return list(map(create_new_symbol, rand_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def expand_terminal(self, symbols: List[Symbol]) -> List:        \n",
    "\n",
    "        # Extract nonterminals\n",
    "        nonterminals = [symbol for symbol in symbols\n",
    "                        if isinstance(symbol, Nonterminal)]\n",
    "            \n",
    "        if not nonterminals:\n",
    "            raise NoNonterminal\n",
    "            \n",
    "        # Expand random nonterminal\n",
    "        expand_ind = np.random.choice(len(nonterminals))\n",
    "        nonterminal = nonterminals[expand_ind]\n",
    "        new_symbols = nonterminal.production()\n",
    "            \n",
    "        # Swap nonterminal with new symbols\n",
    "        nonterminals_processed = 0\n",
    "        for ind in range(len(symbols)):\n",
    "            if isinstance(symbols[ind], Nonterminal):\n",
    "                nonterminals_processed += 1\n",
    "                    \n",
    "                if nonterminals_processed-1 == expand_ind:\n",
    "                        \n",
    "                    # Delete the old nonterminal\n",
    "                    symbols.pop(ind)\n",
    "                                \n",
    "                    # Insert new ones\n",
    "                    symbols = symbols[:ind] + new_symbols + symbols[ind:]\n",
    "\n",
    "        return symbols\n",
    "    \n",
    "    def symbols_to_strings(self, symbols: List[Symbol]) -> List:\n",
    "        return [symbol.symbol for symbol in symbols]\n",
    "        \n",
    "    def gen_terminals(self, start_symbol: Symbol) -> List:\n",
    "        symbols = [start_symbol]\n",
    "        \n",
    "        # Expand until there is any nonterminal in the symbols\n",
    "        while True:\n",
    "            try:\n",
    "                symbols = self.expand_terminal(symbols)\n",
    "            except NoNonterminal:\n",
    "                return self.symbols_to_strings(symbols)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sentence schemas and group them by the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schemas(n_iter: int = 1000, schemas: Dict = {}) -> Dict:\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        start_symbol = Nonterminal('S', NONTERMINALS['S'])\n",
    "        \n",
    "        schema = tuple(gen.gen_terminals(start_symbol))\n",
    "        schema_len = len(schema)\n",
    "        \n",
    "        # Update schemas\n",
    "        if schema_len in schemas:\n",
    "            schemas[schema_len].add(schema)\n",
    "        else:\n",
    "            schemas[schema_len] = {schema}\n",
    "            \n",
    "    # Map sets to tuples to enable drawing\n",
    "    schemas = {key: tuple(val) for key, val in schemas.items()}\n",
    "            \n",
    "    return schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: (('adv:', 'verb:imps', 'subst:pl:acc:f'),\n",
       "  ('verb:imps', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       "  ('adv:', 'verb:imps', 'subst:sg:acc:m1'),\n",
       "  ('verb:imps', 'adj:pl:acc:m2.m3.f.n1.n2.p2.p3', 'subst:pl:acc:f'),\n",
       "  ('adv:', 'adv:', 'verb:imps'),\n",
       "  ('adv:', 'verb:imps', 'subst:pl:acc:n2')),\n",
       " 4: (('adv:', 'adv:', 'verb:imps', 'subst:pl:acc:f'),\n",
       "  ('verb:imps',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'subst:pl:acc:f'),\n",
       "  ('adv:', 'verb:imps', 'adj:pl:acc:m2.m3.f.n1.n2.p2.p3', 'subst:pl:acc:f'),\n",
       "  ('adv:', 'verb:imps', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       "  ('adv:', 'adv:', 'verb:imps', 'subst:sg:acc:m1'),\n",
       "  ('verb:imps', 'adj:sg:acc:m1', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       "  ('adv:', 'verb:imps', 'adj:pl:acc:m2.m3.f.n1.n2.p2.p3', 'subst:pl:acc:n2'),\n",
       "  ('adv:', 'adv:', 'verb:imps', 'subst:pl:acc:n2'))}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas = create_schemas()\n",
    "\n",
    "# Show some schemas\n",
    "dict(list(schemas.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the grammar categories found in the grammar schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolimorfGen:\n",
    "    POLIMORF_PATH = './data/polimorfologik-2.1.txt'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grammar_cats = dict((terminal, [])\n",
    "                                  for terminal in TERMINALS)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.POLIMORF_PATH) as f:\n",
    "            yield from f\n",
    "            \n",
    "    def find_terminal_occ(self, line: str):\n",
    "        \"\"\"\n",
    "        Search for each pattern (terminal)\n",
    "        in the line of the polimorfologik file\n",
    "        \"\"\"\n",
    "        \n",
    "        base, token, grammar_cats = line.split(';')\n",
    "        \n",
    "        for terminal in self.grammar_cats:\n",
    "            pattern = re.compile(terminal)\n",
    "        \n",
    "            if pattern.search(grammar_cats):\n",
    "                self.grammar_cats[terminal].append((base, token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polimorf = PolimorfGen()\n",
    "\n",
    "# Extract the categories\n",
    "for line in polimorf:\n",
    "    polimorf.find_terminal_occ(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sentences of length n without using the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentGen:\n",
    "    def rand_schema(self, n: int) -> Tuple:\n",
    "\n",
    "        # Draw the sentence schema\n",
    "        try:\n",
    "            schemas_n_len = schemas[n]\n",
    "            return random.choice(schemas_n_len)\n",
    "        except KeyError:\n",
    "            raise TooLongSent\n",
    "\n",
    "    def core_sent_gen(self, n: int) -> Tuple:\n",
    "        try:\n",
    "            schema = self.rand_schema(n)\n",
    "            \n",
    "            # Draw the tokens\n",
    "            tokens_with_bases = [random.choice(polimorf.grammar_cats[category])\n",
    "                                 for category in schema]\n",
    "\n",
    "            bases, tokens = list(zip(*tokens_with_bases))\n",
    "\n",
    "            return ' '.join(bases), ' '.join(tokens)\n",
    "        except TooLongSent:\n",
    "            print('Sentence too long')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_gen = SentGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wydłużyć niesłupkowy Arecki', 'wydłużono niesłupkowego Areckiego')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('muszyńsko zużytkować pampasowy nieidealistyczny Tyrawa',\n",
       " 'muszyńsko zużytkowano pampasowego nieidealistycznego Tyrawę')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('niekonstancińsko maszopsko grudzić niejasnopłowy niekabulski mantyctwo',\n",
       " 'niekonstancińsko maszopsko grudzono niejasnopłowe niekabulskie mantyctwa')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence too long\n"
     ]
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Word2Vec struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusGen:\n",
    "    CORPUS_PATH = './data/task3_train_segmented.txt'\n",
    "    \n",
    "    def __init__(self, n_sent):\n",
    "        self.n_sent = n_sent\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.CORPUS_PATH) as f:\n",
    "            for line, _ in zip(f, range(self.n_sent)):\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./data/word2vec.model'):\n",
    "    # Perform the embeddings only during the first session \n",
    "    \n",
    "    sentences = CorpusGen(10_000_000)\n",
    "    model = Word2Vec(sentences, min_count=1)\n",
    "    model.save('./data/word2vec.model')\n",
    "else:\n",
    "    # The model exists\n",
    "    \n",
    "    # Gensim fails in case of loading the model for the second time\n",
    "    try:\n",
    "        model\n",
    "    except NameError:\n",
    "        model = Word2Vec.load('./data/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2640650"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate thematical sentences from the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = (\n",
    "    ('malina', 'koszyk', 'zazdrość', 'morderstwo'),\n",
    "    ('programowanie', 'błąd', 'zmienna', 'deklaracja'),\n",
    "    ('lotniskowiec', 'łódź', 'podwodny',\n",
    "     'tonąć', 'atak', 'torpeda', 'ocean'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicSentGen(SentGen):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def choose_best_token(self, tokens: List, topic: Tuple) -> str:\n",
    "        \n",
    "        # Draw the topic token\n",
    "        topic_token = random.choice(topic)\n",
    "        \n",
    "        # Filter the tokens being in both model and polimorf\n",
    "        common_tokens = [token for (base, token) in tokens\n",
    "                         if token in model.wv.vocab]\n",
    "        \n",
    "        similarity, best_token = max([(model.wv.similarity(topic_token, token),\n",
    "                                       token) for token in common_tokens])\n",
    "        return best_token\n",
    "        \n",
    "    def topic_sent_gen(self, n: int, topic: Tuple, n_to_choose: int = None) -> str:\n",
    "        try:\n",
    "            schema = self.rand_schema(n)\n",
    "            \n",
    "            categories = [polimorf.grammar_cats[category]\n",
    "                          for category in schema]\n",
    "            \n",
    "            topic_sent = [self.choose_best_token(category, topic)\n",
    "                          for category in categories]\n",
    "            \n",
    "            return topic_sent\n",
    "            \n",
    "        except TooLongSent:\n",
    "            print('Sentence too long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sent_gen = TopicSentGen(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['odprzodowo', 'oblatano', 'alianckie', 'żaglowe', 'chiny']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sent_gen.topic_sent_gen(5, TOPICS[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
