{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating thematic sentences from the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List, Tuple, Dict, Callable\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable annoying warnings from gensim\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nontrivial polish language grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "NONTERMINALS = {\n",
    "    'S': [\n",
    "        ('VERB_PHRASE_IMPS',),\n",
    "        ('VERB_PHRASE_IMPS', 'CONJ', 'VERB_PHRASE_IMPS'),\n",
    "        ('NOM_PHRASE_SG_M', 'VERB_PHRASE_SG_FIN_M_TER'),\n",
    "        ('NOM_PHRASE_SG_M', 'VERB_PHRASE_SG_FIN_M_TER',\n",
    "         'CONJ', 'VERB_PHRASE_SG_FIN_M_TER'),\n",
    "        ('NOM_PHRASE_SG_M', 'VERB_PHRASE_SG_FIN_M_TER',\n",
    "         'CONJ', 'NOM_PHRASE_SG_M', 'VERB_PHRASE_SG_FIN_M_TER'),\n",
    "    ],\n",
    "    'VERB_PHRASE_IMPS': [\n",
    "        ('VERB_IMPS',),\n",
    "        ('ADV', 'VERB_IMPS'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_SG_M1'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_PL_F'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_PL_N2'),\n",
    "        ('ADV', 'VERB_IMPS', 'ACC_PHRASE_SG_M1'),\n",
    "        ('ADV', 'VERB_IMPS', 'ACC_PHRASE_PL_F'),\n",
    "        ('ADV', 'VERB_IMPS', 'ACC_PHRASE_PL_N2'),\n",
    "        \n",
    "    ],\n",
    "    'VERB_PHRASE_SG_FIN_M_TER': [\n",
    "        ('VERB_SG_FIN_M_TER',),\n",
    "        ('ADV', 'VERB_SG_FIN_M_TER'),\n",
    "        ('VERB_SG_FIN_M_TER', 'ACC_PHRASE_SG_M1'),\n",
    "        ('VERB_SG_FIN_M_TER', 'ACC_PHRASE_PL_F'),\n",
    "        ('VERB_SG_FIN_M_TER', 'ACC_PHRASE_PL_N2'),\n",
    "        ('ADV', 'VERB_SG_FIN_M_TER', 'ACC_PHRASE_SG_M1'),\n",
    "        ('ADV', 'VERB_SG_FIN_M_TER', 'ACC_PHRASE_PL_F'),\n",
    "        ('ADV', 'VERB_SG_FIN_M_TER', 'ACC_PHRASE_PL_N2'),\n",
    "    ],\n",
    "    'NOM_PHRASE_SG_M': [\n",
    "        ('SUBST_SG_NOM_M',),\n",
    "        ('SUBST_SG_NOM_M', 'PREP_ACC_PHRASE'),\n",
    "        ('ADJ_PHRASE_SG_NOM_M', 'SUBST_SG_NOM_M'),\n",
    "        ('ADJ_PHRASE_SG_NOM_M', 'SUBST_SG_NOM_M', 'PREP_ACC_PHRASE'),\n",
    "    ],\n",
    "    'ACC_PHRASE_SG_M1': [\n",
    "        ('SUBST_SG_ACC_M1',),\n",
    "        ('ADJ_PHRASE_SG_ACC_M1', 'SUBST_SG_ACC_M1',),\n",
    "    ],\n",
    "    'ACC_PHRASE_PL_F': [\n",
    "        ('SUBST_PL_ACC_F',),\n",
    "        ('ADJ_PHRASE_PL_ACC_F_N2', 'SUBST_PL_ACC_F'),\n",
    "    ],\n",
    "    'ACC_PHRASE_PL_N2': [\n",
    "        ('SUBST_PL_ACC_N2',),\n",
    "        ('ADJ_PHRASE_PL_ACC_F_N2', 'SUBST_PL_ACC_N2'),\n",
    "    ],\n",
    "    'ADJ_PHRASE_SG_NOM_M': [\n",
    "        ('ADJ_SG_NOM_M',),\n",
    "        ('ADJ_SG_NOM_M', 'ADJ_SG_NOM_M'),\n",
    "    ],\n",
    "    'ADJ_PHRASE_SG_ACC_M1': [\n",
    "        ('ADJ_SG_ACC_M1',),\n",
    "        ('ADJ_SG_ACC_M1', 'ADJ_SG_ACC_M1'),\n",
    "    ],\n",
    "    'ADJ_PHRASE_PL_ACC_F_N2': [\n",
    "        ('ADJ_PL_ACC_F_N2',),\n",
    "        ('ADJ_PL_ACC_F_N2', 'ADJ_PL_ACC_F_N2'),\n",
    "    ],\n",
    "    'PREP_ACC_PHRASE': [\n",
    "        ('PREP_ACC', 'ACC_PHRASE_SG_M1'),\n",
    "        ('PREP_ACC', 'ACC_PHRASE_PL_F'),\n",
    "        ('PREP_ACC', 'ACC_PHRASE_PL_N2'),\n",
    "    ],\n",
    "    \n",
    "    # Productions with terminals\n",
    "    'VERB_SG_FIN_M_TER': [('verb:fin:sg:ter.*:refl',)],\n",
    "    'VERB_IMPS': [('verb:imps',)],\n",
    "    'SUBST_SG_NOM_M': [('subst:sg:nom:m',)],\n",
    "    'SUBST_SG_ACC_M1': [('subst:sg:acc:m1',)],\n",
    "    'SUBST_PL_ACC_F': [('subst:pl:acc:f',)],\n",
    "    'SUBST_PL_ACC_N2': [('subst:pl:acc:n2',)],\n",
    "    'ADJ_SG_NOM_M': [('adj:sg:nom.voc:m1.m2.m3',)],\n",
    "    'ADJ_SG_ACC_M1': [('adj:sg:acc:m1',)],\n",
    "    'ADJ_PL_ACC_F_N2': [('adj:pl:acc:m2.m3.f.n1.n2.p2.p3',)],\n",
    "    'ADV': [('adv:',)],\n",
    "    'PREP_ACC': [('prep:acc',)],\n",
    "    'CONJ': [('^conj$',)],\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINALS = (\n",
    "    'verb:fin:sg:ter.*:refl',\n",
    "    'verb:imps',\n",
    "    'subst:sg:nom:m',\n",
    "    'subst:sg:acc:m1',\n",
    "    'subst:pl:acc:f',\n",
    "    'subst:pl:acc:n2',\n",
    "    'adj:sg:nom.voc:m1.m2.m3',\n",
    "    'adj:sg:acc:m1',\n",
    "    'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
    "    'adv:',\n",
    "    'prep:acc',\n",
    "    '^conj$',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoNonterminal(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TooLongSent(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symbol:\n",
    "    def __init__(self, symbol: str):\n",
    "        self.symbol = symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Terminal(Symbol):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nonterminal(Symbol):\n",
    "    def __init__(self, symbol: str, productions: List[Symbol]):\n",
    "        super().__init__(symbol)\n",
    "        self.productions = productions\n",
    "        \n",
    "    def production(self) -> Tuple[Symbol]:\n",
    "\n",
    "        def create_new_symbol(symbol) -> Symbol:\n",
    "           if symbol in NONTERMINALS:\n",
    "               return Nonterminal(symbol, NONTERMINALS[symbol])\n",
    "           else:\n",
    "               return Terminal(symbol)\n",
    "        \n",
    "        # Draw the production\n",
    "        rand_prod_ind = np.random.choice(len(self.productions))\n",
    "        rand_prod = self.productions[rand_prod_ind]\n",
    "            \n",
    "        return list(map(create_new_symbol, rand_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def expand_terminal(self, symbols: List[Symbol]) -> List:        \n",
    "\n",
    "        # Extract nonterminals\n",
    "        nonterminals = [symbol for symbol in symbols\n",
    "                        if isinstance(symbol, Nonterminal)]\n",
    "            \n",
    "        if not nonterminals:\n",
    "            raise NoNonterminal\n",
    "            \n",
    "        # Expand random nonterminal\n",
    "        expand_ind = np.random.choice(len(nonterminals))\n",
    "        nonterminal = nonterminals[expand_ind]\n",
    "        new_symbols = nonterminal.production()\n",
    "            \n",
    "        # Swap nonterminal with new symbols\n",
    "        nonterminals_processed = 0\n",
    "        for ind in range(len(symbols)):\n",
    "            if isinstance(symbols[ind], Nonterminal):\n",
    "                nonterminals_processed += 1\n",
    "                    \n",
    "                if nonterminals_processed-1 == expand_ind:\n",
    "                        \n",
    "                    # Delete the old nonterminal\n",
    "                    symbols.pop(ind)\n",
    "                                \n",
    "                    # Insert new ones\n",
    "                    symbols = symbols[:ind] + new_symbols + symbols[ind:]\n",
    "\n",
    "        return symbols\n",
    "    \n",
    "    def symbols_to_strings(self, symbols: List[Symbol]) -> List:\n",
    "        return [symbol.symbol for symbol in symbols]\n",
    "        \n",
    "    def gen_terminals(self, start_symbol: Symbol) -> List:\n",
    "        symbols = [start_symbol]\n",
    "        \n",
    "        # Expand until there is any nonterminal in the symbols\n",
    "        while True:\n",
    "            try:\n",
    "                symbols = self.expand_terminal(symbols)\n",
    "            except NoNonterminal:\n",
    "                return self.symbols_to_strings(symbols)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sentence schemas and group them by the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schemas(n_iter: int = 10000, schemas: Dict = {}) -> Dict:\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        start_symbol = Nonterminal('S', NONTERMINALS['S'])\n",
    "        \n",
    "        schema = tuple(gen.gen_terminals(start_symbol))\n",
    "        schema_len = len(schema)\n",
    "        \n",
    "        # Update schemas\n",
    "        if schema_len in schemas:\n",
    "            schemas[schema_len].add(schema)\n",
    "        else:\n",
    "            schemas[schema_len] = {schema}\n",
    "            \n",
    "    # Map sets to tuples to enable drawing\n",
    "    schemas = {key: tuple(val) for key, val in schemas.items()}\n",
    "            \n",
    "    return schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('verb:imps', 'adj:pl:acc:m2.m3.f.n1.n2.p2.p3', 'subst:pl:acc:n2'),\n",
       " ('subst:sg:nom:m', 'adv:', 'verb:fin:sg:ter.*:refl'),\n",
       " ('adv:', 'verb:imps', 'subst:pl:acc:f'),\n",
       " ('subst:sg:nom:m', 'verb:fin:sg:ter.*:refl', 'subst:sg:acc:m1'),\n",
       " ('verb:imps', 'adj:pl:acc:m2.m3.f.n1.n2.p2.p3', 'subst:pl:acc:f'),\n",
       " ('verb:imps', '^conj$', 'verb:imps'),\n",
       " ('subst:sg:nom:m', 'verb:fin:sg:ter.*:refl', 'subst:pl:acc:f'),\n",
       " ('adv:', 'verb:imps', 'subst:pl:acc:n2'),\n",
       " ('adj:sg:nom.voc:m1.m2.m3', 'subst:sg:nom:m', 'verb:fin:sg:ter.*:refl'),\n",
       " ('verb:imps', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       " ('subst:sg:nom:m', 'verb:fin:sg:ter.*:refl', 'subst:pl:acc:n2'),\n",
       " ('adv:', 'verb:imps', 'subst:sg:acc:m1'))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas = create_schemas()\n",
    "\n",
    "# Show some schemas\n",
    "schemas[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the grammar categories found in the grammar schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolimorfGen:\n",
    "    POLIMORF_PATH = './data/polimorfologik-2.1.txt'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grammar_cats = dict((terminal, [])\n",
    "                                  for terminal in TERMINALS)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.POLIMORF_PATH) as f:\n",
    "            yield from f\n",
    "            \n",
    "    def find_terminal_occ(self, line: str):\n",
    "        \"\"\"\n",
    "        Search for each pattern (terminal)\n",
    "        in the line of the polimorfologik file\n",
    "        \"\"\"\n",
    "        \n",
    "        base, token, grammar_cats = line.split(';')\n",
    "        \n",
    "        for terminal in self.grammar_cats:\n",
    "            pattern = re.compile(terminal)\n",
    "        \n",
    "            if pattern.search(grammar_cats):\n",
    "                self.grammar_cats[terminal].append((base, token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polimorf = PolimorfGen()\n",
    "\n",
    "# Extract the categories\n",
    "for line in polimorf:\n",
    "    polimorf.find_terminal_occ(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sentences of length n without using the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentGen:\n",
    "    def rand_schema(self, n: int) -> Tuple:\n",
    "\n",
    "        # Draw the sentence schema\n",
    "        try:\n",
    "            schemas_n_len = schemas[n]\n",
    "            return random.choice(schemas_n_len)\n",
    "        except KeyError:\n",
    "            raise TooLongSent\n",
    "\n",
    "    def core_sent_gen(self, n: int) -> Tuple:\n",
    "        try:\n",
    "            schema = self.rand_schema(n)\n",
    "            \n",
    "            # Draw the tokens\n",
    "            tokens_with_bases = [random.choice(polimorf.grammar_cats[category])\n",
    "                                 for category in schema]\n",
    "\n",
    "            bases, tokens = list(zip(*tokens_with_bases))\n",
    "\n",
    "            return ' '.join(tokens)\n",
    "        except TooLongSent:\n",
    "            print('Sentence too long')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_gen = SentGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pozaracjonalny kresowianin przegęści wysmuklałego alowca'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dwuramiennik upija lecz kirsch pogrubia kanapska'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'margiel poprzeze konkatenacje narzyna wszakżeż zaszachuje sprawstwa'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kunowaty niegorzkokwaśny wierszorób za afroazjatyckiego Gałęzowskiego niemasowo stymuluje kompozytorstwa'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intrateluryczny trzonowiec poprzez nansenowskiego śmiechulskiego Bileckiego nielibrewilsko remonstruje antywęgierskiego wiertnika póty niekarkonoski Kunowski ponad przełącznikowe niesubarktyczne potomstwa szacunkowo uchowa miłosierdzia'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence too long\n"
     ]
    }
   ],
   "source": [
    "sent_gen.core_sent_gen(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Word2Vec struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusGen:\n",
    "    CORPUS_PATH = './data/task3_train_segmented.txt'\n",
    "    \n",
    "    def __init__(self, n_sent):\n",
    "        self.n_sent = n_sent\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.CORPUS_PATH) as f:\n",
    "            for line, _ in zip(f, range(self.n_sent)):\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./data/word2vec.model'):\n",
    "    # Perform the embeddings only during the first session \n",
    "    \n",
    "    sentences = CorpusGen(10_000_000)\n",
    "    model = Word2Vec(sentences, min_count=1)\n",
    "    model.save('./data/word2vec.model')\n",
    "else:\n",
    "    # The model exists\n",
    "    \n",
    "    # Gensim fails in case of loading the model for the second time\n",
    "    try:\n",
    "        model\n",
    "    except NameError:\n",
    "        model = Word2Vec.load('./data/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2640650"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate thematical sentences from the grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = (\n",
    "    ('malina', 'koszyk', 'zazdrość', 'morderstwo'),\n",
    "    ('programowanie', 'błąd', 'zmienna', 'deklaracja'),\n",
    "    ('lotniskowiec', 'łódź', 'podwodny',\n",
    "     'tonąć', 'atak', 'torpeda', 'ocean'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicSentGen(SentGen):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def choose_best_token(self, tokens: List, topic: Tuple) -> str:\n",
    "        \n",
    "        # Draw the topic token\n",
    "        topic_token = random.choice(topic)\n",
    "        \n",
    "        base_token_similarities = {pair: 0 for pair in tokens}\n",
    "    \n",
    "        def update_sims(pair: Tuple, token: str) -> Dict:\n",
    "            if token in self.model.wv.vocab:\n",
    "                base_token_similarities[pair] +=\\\n",
    "                    model.wv.similarity(token, topic_token)\n",
    "    \n",
    "        # For each pair similarity is a sum of\n",
    "        # similarity(base, topic_token) and similarity(token, topic_word)\n",
    "        for base, token in base_token_similarities:\n",
    "            update_sims((base, token), base)\n",
    "            update_sims((base, token), token)\n",
    "            \n",
    "        (best_base, best_token), sim = max(base_token_similarities.items(),\n",
    "                                           key=itemgetter(1))\n",
    "                    \n",
    "        return best_token, sim\n",
    "        \n",
    "    def gen(self, n: int, topic: Tuple,\n",
    "            n_to_choose: int = 1000) -> str:\n",
    "        try:\n",
    "            schema = self.rand_schema(n)\n",
    "            \n",
    "            categories = [random.choices(polimorf.grammar_cats[category],\n",
    "                                         k=min(n_to_choose,\n",
    "                                               len(polimorf.grammar_cats[category])))\n",
    "                          for category in schema]\n",
    "            \n",
    "            topic_sent_sims = [self.choose_best_token(category, topic)\n",
    "                               for category in categories]\n",
    "            \n",
    "            topic_sent, sims = list(zip(*topic_sent_sims))\n",
    "            \n",
    "            return list(topic_sent), np.mean(sims)\n",
    "            \n",
    "        except TooLongSent:\n",
    "            print('Sentence too long')\n",
    "            \n",
    "    def gen_n_times(self, n: int, topic: Tuple, n_to_choose: int = 100,\n",
    "                    n_times: int = 1000) -> List:\n",
    "        \n",
    "        sents_sims = [self.gen(n, topic, n_to_choose)\n",
    "                      for _ in range(n_times)]\n",
    "        \n",
    "        # Return also sims for further processing\n",
    "        return sents_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sent_gen = TopicSentGen(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['prześladowano', 'ergo', 'wlano', 'dyniowe', 'bagietki'], 0.8558836877346039)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sent_gen.gen(5, TOPICS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['magnetohydrodynamiczny',\n",
       "  'trawers',\n",
       "  'popod',\n",
       "  'truskawkowe',\n",
       "  'mokra',\n",
       "  'czytelniczo',\n",
       "  'wodzi',\n",
       "  'jajcarskie',\n",
       "  'torpeda',\n",
       "  'jakoż',\n",
       "  'kamikaze',\n",
       "  'popod',\n",
       "  'kanapy',\n",
       "  'bezapelacyjnie',\n",
       "  'oblatuje'],\n",
       " 0.9044725775718689)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sent_gen.gen(15, TOPICS[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['czerwonobrązowy',\n",
       "   'filc',\n",
       "   'oczernia',\n",
       "   'ustawienia',\n",
       "   'oraz',\n",
       "   'kapsyd',\n",
       "   'zadeklaruje',\n",
       "   'utrudnienia'],\n",
       "  0.594270022585988),\n",
       " (['tępy',\n",
       "   'pekari',\n",
       "   'co',\n",
       "   'kompatybilne',\n",
       "   'ćwiczenia',\n",
       "   'teoretycznie',\n",
       "   'pociupcia',\n",
       "   'obrażenia'],\n",
       "  0.5940124187618494)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_sent_gen.gen_n_times(8, TOPICS[1], n_times=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the best topic sent with Positive Pointwise Mutual Information (PPMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unigrams and bigrams structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGrams:\n",
    "\n",
    "    DATA_PATH = './data/poleval_2grams.txt'\n",
    "\n",
    "    def create_bigrams_unigrams(self, k: int = 10) -> Tuple:\n",
    "        \n",
    "        unigrams, bigrams = {}, {}\n",
    "        \n",
    "        def update_unigrams(token: str, freq: str) -> None:\n",
    "            if token in unigrams:\n",
    "                unigrams[token] += int(freq)\n",
    "            else:\n",
    "                unigrams[token] = int(freq)\n",
    "                \n",
    "        def update_bigrams(predecesor: str, successor: str,\n",
    "                           freq: str) -> None:\n",
    "            bigrams[(predecesor, successor)] = int(freq)\n",
    "\n",
    "        with open(self.DATA_PATH) as poleval:\n",
    "            for line in poleval:\n",
    "                freq, predecesor, successor = line.split()\n",
    "\n",
    "                # Update bigrams ans unigrams\n",
    "                if int(freq) >= k:\n",
    "                    update_bigrams(predecesor, successor, freq)\n",
    "                    update_unigrams(predecesor, freq)\n",
    "                    update_unigrams(successor, freq)\n",
    "\n",
    "        return unigrams, bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = NGrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = ngrams.create_bigrams_unigrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 52290593,\n",
       " '.': 40527277,\n",
       " 'rozdrobniona': 151,\n",
       " 'sieć': 23967,\n",
       " 'świadectwem': 1850}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of unigrams\n",
    "dict(list(unigrams.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('nastąpiło', 'przedawnienie'): 32,\n",
       " ('podzielają', 'pogląd'): 32,\n",
       " ('rozdrobniona', 'sieć'): 11,\n",
       " ('świadectwem', ','): 87,\n",
       " ('świadectwem', '.'): 41}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of bigrams\n",
    "dict(list(bigrams.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPMI:\n",
    "    def __init__(self, model, unigrams: Dict, bigrams: Dict):\n",
    "        self.model = model\n",
    "        self.unigrams = unigrams\n",
    "        self.bigrams = bigrams\n",
    "        self.all_unigrams = sum(unigrams.values())\n",
    "    \n",
    "    def measure_ppmi(self, sentence: List) -> float:\n",
    "        predecesors = sentence.copy()\n",
    "        successors = sentence.copy()\n",
    "        \n",
    "        predecesors.insert(0, '<BOS>')\n",
    "        successors.append('<EOS>')\n",
    "        \n",
    "        def ppmi(predecesor: str, successor: str) -> float:\n",
    "            numerator = bigrams.get((predecesor, successor),\n",
    "                                    1.) * self.all_unigrams / 2\n",
    "            denominator = unigrams.get(predecesor, 1.) *\\\n",
    "                          unigrams.get(successor, 1.)\n",
    "            \n",
    "            # POSITIVE pointwise mutual information\n",
    "            return max(np.log(numerator / denominator), 0)\n",
    "        \n",
    "        sent_bigrams = list(zip(predecesors, successors))\n",
    "        \n",
    "        # Sum of PPMI of each bigram in the sent\n",
    "        ppmi = sum([ppmi(predecesor, successor)\n",
    "                   for predecesor, successor in sent_bigrams])\n",
    "        \n",
    "        return ppmi\n",
    "    \n",
    "    def choose_highest_ppmi(self, sents_sims: List) -> str:\n",
    "        ppmi_sents = [(self.measure_ppmi(sent), sent)\n",
    "                      for sent, sim in sents_sims]\n",
    "        \n",
    "        _, best_sent = max(ppmi_sents)\n",
    "        \n",
    "        return ' '.join(best_sent).capitalize()\n",
    "    \n",
    "    def choose_highest_sim(self, sents_sims: List) -> str:\n",
    "        best_sent = max(sents_sims, key=itemgetter(1))[0]\n",
    "        \n",
    "        return ' '.join(best_sent).capitalize()\n",
    "    \n",
    "    def choose_highest_ppmi_sim(self, sents_sims: List) -> str:\n",
    "        \n",
    "        # Choose the sent with the highest geometric mean\n",
    "        # of ppmi and mean token similarity\n",
    "        ppmi_sim_sents = [(np.sqrt(self.measure_ppmi(sent) * sim), sent)\n",
    "                          for sent, sim in sents_sims]\n",
    "        \n",
    "        _, best_sent = max(ppmi_sim_sents)\n",
    "        \n",
    "        return ' '.join(best_sent).capitalize()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random sequence of topics and coresponding sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi = PPMI(model, unigrams, bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_best_sent(optimizer: Callable[[List], str],\n",
    "                  topic: str, sent_len: int) -> str:\n",
    "    \n",
    "    return optimizer(topic_sent_gen.gen_n_times(sent_len, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test(n_sents: int, sent_len: int,\n",
    "              optimizer: Callable[[List], str]):\n",
    "    \n",
    "    # Rand topics\n",
    "    topics = random.choices(range(3), k=n_sents)\n",
    "    \n",
    "    # Gen sentences\n",
    "    for topic_ind in topics:\n",
    "        print(gen_best_sent(optimizer, TOPICS[topic_ind], sent_len))\n",
    "        \n",
    "    # Return topics to allow checking predictions\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may check the predictions\n",
    "def check_predictions(real: List, preds: List) -> float:\n",
    "    try:\n",
    "        print(accuracy_score(real, preds))\n",
    "    except ValueError:\n",
    "        print(\"Preds and list dimensions must match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('malina', 'koszyk', 'zazdrość', 'morderstwo'),\n",
       " ('programowanie', 'błąd', 'zmienna', 'deklaracja'),\n",
       " ('lotniskowiec', 'łódź', 'podwodny', 'tonąć', 'atak', 'torpeda', 'ocean'))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remind the topics\n",
    "TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nieulotny przywęglowy diploid prawostronnie zdezorientuje póty łaje\n",
      "Konwerter popod niegonokokowe spodnium dalekosiężnie wyżyłuje rozwarcia\n",
      "Dozorowano dyfuzyjne einsteinowskie sędzie póty zestalono tępiciela\n",
      "Żbik popod dalekonośnego czapkę podstępniej zaklei tatarzyna\n",
      "Tożsamościowo opuszczano owacyjne bahamy alić miłowano beri-beri\n"
     ]
    }
   ],
   "source": [
    "real_short_ppmi = make_test(5, 7, ppmi.choose_highest_ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds and list dimensions must match\n"
     ]
    }
   ],
   "source": [
    "check_predictions([], real_short_ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Świdwinianin popod mantuańskie zadry niehumanitarnie buforuje logiczniejszego poszczepiennego załadowcę póty wyceni fabuły\n",
      "Elektrozawór skroś dopinki kardynalnie ocala mściwe różnobarwne homofobie jakoż użądli wybielające samobójstwa\n",
      "Nieustraszony ustaszowski anorak popod diagonalne benity krztusi terazzo alić nieustępliwie pasteryzuje drakulę\n",
      "Pancernik popod półgąsienicowego kiereńskiego złociście odkręci alić suprematystyczny kilkukilogramowy firmament odreaguje hipotrepsje\n",
      "Lamaistyczny akceptacyjny porada obezwładniająco wypruje siłki oraz organicznie uwarunkuje odczasownikowe jednowymiarowe kooperacje\n"
     ]
    }
   ],
   "source": [
    "real_long_ppmi = make_test(5, 12, ppmi.choose_highest_ppmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds and list dimensions must match\n"
     ]
    }
   ],
   "source": [
    "check_predictions([], real_long_ppmi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gruczołowaty rytualny rozmaryn sennie dopcha duszące sitowia\n",
      "Kauczukowy sandałowiec popod dorsza wyparowuje chłodnicze morderstwa\n",
      "Kolczasty sezamowy miot akustycznie kłuje wróble mrowienia\n",
      "Pomiarowy odcinkowy niejadek jonizuje heterozygotycznego atmosferycznego mausera\n",
      "Dwukwiatowy koci dziewięciornik złociście obnaży skóropodobne naturalności\n"
     ]
    }
   ],
   "source": [
    "real_short_sim = make_test(5, 7, ppmi.choose_highest_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds and list dimensions must match\n"
     ]
    }
   ],
   "source": [
    "check_predictions([], real_short_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kompozytowy filipiński maszt trojako posili przegubowe igiełki tedy zagon rozwala ascetyczne plazmy\n",
      "Prosty podsłuch w jonowe złącza przeliczalnie alergizuje spawania póty stretch naskrobie kiepskości\n",
      "Liliowy ślimak melodyjnie wymoczy palmowego wisusa ali pięciornik niebiańsko gryzie lejkowate dżdżownice\n",
      "Zoom temu rejestratora aktywuje optyczne naprężenia atoli boa poprzez pojemniejsze serwa skala\n",
      "Unix w domniemania zrezygnowanie wymiata elektrochemiczne naświetlania ergo shareware poprzez jądra rozciąga\n"
     ]
    }
   ],
   "source": [
    "real_long_sim = make_test(5, 12, ppmi.choose_highest_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds and list dimensions must match\n"
     ]
    }
   ],
   "source": [
    "check_predictions([], real_long_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined PPMI and similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oktawowy tyciusieńki kompres popod pojmanego urodziwie wysmaruje\n",
      "Vhs schłodzi korekcje póty dokupuje płowoszare ai\n",
      "Pianka odkurzy fondue alić szkot skuma pahlawiego\n",
      "Gidelski jednoręczny smętek nagradza sytego fluorowodorowego kapustę\n",
      "Hełmiasty bezbarwny melon popod konkubenta zwymyśla ambicje\n"
     ]
    }
   ],
   "source": [
    "real_short_ppmi_sim = make_test(5, 7, ppmi.choose_highest_ppmi_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds and list dimensions must match\n"
     ]
    }
   ],
   "source": [
    "check_predictions([], real_short_ppmi_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odłamkowy uraz dostojnie zmagazynuje bateryjne siksy alić hakowato skonsoliduje dostawcze magnetohydrodynamiczne hamilton\n",
      "Amunicyjny legitymizm niespodzianie dokuje cicero tedy ostrzew nieprzewidzianie pompuje czułkowe tęższe haiti\n",
      "Ubój nabłyszczy orzechodajnego biathlonistę ali łososiowy capote mongolsko sklei listkowe lojalne utarczki\n",
      "Egoizm bateryjnie wytrze smutnego kurczaka vel jednobarwny podwładny natarczywie pasteryzuje niezniszczonego rekina\n",
      "Zmizerniały oplot legnicko potroi estymatora póty lśniący przyzywający duran certyfikuje oświeceńsze zalecenia\n"
     ]
    }
   ],
   "source": [
    "real_long_ppmi_sim = make_test(5, 12, ppmi.choose_highest_ppmi_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds and list dimensions must match\n"
     ]
    }
   ],
   "source": [
    "check_predictions([], real_long_ppmi_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
