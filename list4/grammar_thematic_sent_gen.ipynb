{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from typing import List, Tuple, Dict\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define nontrivial polish language grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "NONTERMINALS = {\n",
    "    'S': [\n",
    "        ('VERB_PHRASE',),\n",
    "    ],\n",
    "    'VERB_PHRASE': [\n",
    "        ('VERB_IMPS',),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_SG_M1'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_PL_F'),\n",
    "        ('VERB_IMPS', 'ACC_PHRASE_PL_N1'),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS', 'ACC_PHRASE_SG_M1'),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS', 'ACC_PHRASE_PL_F'),\n",
    "        ('ADV_PHRASE', 'VERB_IMPS', 'ACC_PHRASE_PL_N2'),\n",
    "        \n",
    "    ],\n",
    "    'ADV_PHRASE': [\n",
    "        ('ADV',),\n",
    "        ('ADV', 'ADV',),\n",
    "    ],\n",
    "    'ACC_PHRASE_SG_M1': [\n",
    "        ('SUBST_SG_ACC_M1',),\n",
    "        ('ADJ_PHRASE_SG_ACC_M1', 'SUBST_SG_ACC_M1',),\n",
    "    ],\n",
    "    'ACC_PHRASE_PL_F': [\n",
    "        ('SUBST_PL_ACC_F',),\n",
    "        ('ADJ_PHRASE_PL_ACC_F_N2', 'SUBST_PL_ACC_F'),\n",
    "    ],\n",
    "    'ACC_PHRASE_PL_N2': [\n",
    "        ('SUBST_PL_ACC_N2',),\n",
    "        ('ADJ_PHRASE_PL_ACC_F_N2', 'SUBST_PL_ACC_N2'),\n",
    "    ],\n",
    "    'ADJ_PHRASE_SG_ACC_M1': [\n",
    "        ('ADJ_SG_ACC_M1',),\n",
    "        ('ADJ_SG_ACC_M1', 'ADJ_SG_ACC_M1'),\n",
    "    ],\n",
    "    'ADJ_PHRASE_PL_ACC_F_N2': [\n",
    "        ('ADJ_PL_ACC_F_N2',),\n",
    "        ('ADJ_PL_ACC_F_N2', 'ADJ_PL_ACC_F_N2'),\n",
    "    ],\n",
    "    \n",
    "    # Productions with terminals\n",
    "    'VERB_IMPS': [\n",
    "        ('verb:imps',),\n",
    "    ],\n",
    "    'SUBST_SG_ACC_M1': [\n",
    "        ('subst:sg:acc:m1',),\n",
    "    ],\n",
    "    'SUBST_PL_ACC_F': [\n",
    "        ('subst:pl:acc:f',),\n",
    "    ],\n",
    "    'SUBST_PL_ACC_N2': [\n",
    "        ('subst:pl:acc:n2',),\n",
    "    ],\n",
    "    'ADJ_SG_ACC_M1': [\n",
    "        ('adj:sg:acc:m1',),\n",
    "    ],\n",
    "    'ADJ_PL_ACC_F_N2': [\n",
    "        ('adj:pl:acc:m2.m3.f.n1.n2.p2.p3',),\n",
    "    ],\n",
    "    'ADV': [\n",
    "        ('adv:',),\n",
    "    ]\n",
    "}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMINALS = (\n",
    "    'verb:imps',\n",
    "    'subst:sg:acc:m1',\n",
    "    'subst:pl:acc:f',\n",
    "    'subst:pl:acc:n2',\n",
    "    'adj:sg:acc:m1',\n",
    "    'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
    "    'adv:',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoNonterminal(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Symbol:\n",
    "    def __init__(self, symbol: str):\n",
    "        self.symbol = symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Terminal(Symbol):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nonterminal(Symbol):\n",
    "    def __init__(self, symbol: str, productions: List[Symbol]):\n",
    "        super().__init__(symbol)\n",
    "        self.productions = productions\n",
    "        \n",
    "    def production(self) -> Tuple[Symbol]:\n",
    "\n",
    "        def create_new_symbol(symbol) -> Symbol:\n",
    "           if symbol in NONTERMINALS:\n",
    "               return Nonterminal(symbol, NONTERMINALS[symbol])\n",
    "           else:\n",
    "               return Terminal(symbol)\n",
    "        \n",
    "        # Draw the production\n",
    "        rand_prod_ind = np.random.choice(len(self.productions))\n",
    "        rand_prod = self.productions[rand_prod_ind]\n",
    "            \n",
    "        return list(map(create_new_symbol, rand_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    \n",
    "    def expand_terminal(self, symbols: List[Symbol]) -> List:        \n",
    "\n",
    "        # Extract nonterminals\n",
    "        nonterminals = [symbol for symbol in symbols\n",
    "                        if isinstance(symbol, Nonterminal)]\n",
    "            \n",
    "        if not nonterminals:\n",
    "            raise NoNonterminal\n",
    "            \n",
    "        # Expand random nonterminal\n",
    "        expand_ind = np.random.choice(len(nonterminals))\n",
    "        nonterminal = nonterminals[expand_ind]\n",
    "        new_symbols = nonterminal.production()\n",
    "            \n",
    "        # Swap nonterminal with new symbols\n",
    "        nonterminals_processed = 0\n",
    "        for ind in range(len(symbols)):\n",
    "            if isinstance(symbols[ind], Nonterminal):\n",
    "                nonterminals_processed += 1\n",
    "                    \n",
    "                if nonterminals_processed-1 == expand_ind:\n",
    "                        \n",
    "                    # Delete the old nonterminal\n",
    "                    symbols.pop(ind)\n",
    "                                \n",
    "                    # Insert new ones\n",
    "                    symbols = symbols[:ind] + new_symbols + symbols[ind:]\n",
    "\n",
    "        return symbols\n",
    "    \n",
    "    def symbols_to_strings(self, symbols: List[Symbol]) -> List:\n",
    "        return [symbol.symbol for symbol in symbols]\n",
    "        \n",
    "    def gen_terminals(self, start_symbol: Symbol) -> List:\n",
    "        symbols = [start_symbol]\n",
    "        \n",
    "        # Expand until there is any nonterminal in the symbols\n",
    "        while True:\n",
    "            try:\n",
    "                symbols = self.expand_terminal(symbols)\n",
    "            except NoNonterminal:\n",
    "                return self.symbols_to_strings(symbols)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sentence schemas and group them by the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schemas(n_iter: int = 1000, schemas: Dict = {}) -> Dict:\n",
    "    for i in range(n_iter):\n",
    "        \n",
    "        start_symbol = Nonterminal('S', NONTERMINALS['S'])\n",
    "        \n",
    "        schema = tuple(gen.gen_terminals(start_symbol))\n",
    "        schema_len = len(schema)\n",
    "        \n",
    "        # Update schemas\n",
    "        if schema_len in schemas:\n",
    "            schemas[schema_len].add(schema)\n",
    "        else:\n",
    "            schemas[schema_len] = {schema}\n",
    "            \n",
    "    # Map sets to tuples to enable drawing\n",
    "    schemas = {key: tuple(val) for key, val in schemas.items()}\n",
    "            \n",
    "    return schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: (('adv:', 'adv:', 'verb:imps', 'subst:sg:acc:m1'),\n",
       "  ('verb:imps', 'adj:sg:acc:m1', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       "  ('adv:', 'adv:', 'verb:imps', 'subst:pl:acc:n2'),\n",
       "  ('adv:', 'adv:', 'verb:imps', 'subst:pl:acc:f'),\n",
       "  ('verb:imps',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'subst:pl:acc:f'),\n",
       "  ('adv:', 'verb:imps', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       "  ('adv:', 'verb:imps', 'adj:pl:acc:m2.m3.f.n1.n2.p2.p3', 'subst:pl:acc:n2'),\n",
       "  ('adv:', 'verb:imps', 'adj:pl:acc:m2.m3.f.n1.n2.p2.p3', 'subst:pl:acc:f')),\n",
       " 5: (('adv:',\n",
       "   'verb:imps',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'subst:pl:acc:f'),\n",
       "  ('adv:', 'verb:imps', 'adj:sg:acc:m1', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       "  ('adv:',\n",
       "   'adv:',\n",
       "   'verb:imps',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'subst:pl:acc:f'),\n",
       "  ('adv:',\n",
       "   'adv:',\n",
       "   'verb:imps',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'subst:pl:acc:n2'),\n",
       "  ('adv:', 'adv:', 'verb:imps', 'adj:sg:acc:m1', 'subst:sg:acc:m1'),\n",
       "  ('adv:',\n",
       "   'verb:imps',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'adj:pl:acc:m2.m3.f.n1.n2.p2.p3',\n",
       "   'subst:pl:acc:n2'))}"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas = create_schemas()\n",
    "\n",
    "# Show some schemas\n",
    "dict(list(schemas.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the grammar categories found in the grammar schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolimorfGen:\n",
    "    POLIMORF_PATH = './data/polimorfologik-2.1.txt'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grammar_cats = dict((terminal, [])\n",
    "                                  for terminal in TERMINALS)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.POLIMORF_PATH) as f:\n",
    "            yield from f\n",
    "            \n",
    "    def find_terminal_occ(self, line: str):\n",
    "        \"\"\"\n",
    "        Search for each pattern (terminal)\n",
    "        in the line of the polimorfologik file\n",
    "        \"\"\"\n",
    "        \n",
    "        base, token, grammar_cats = line.split(';')\n",
    "        \n",
    "        for terminal in self.grammar_cats:\n",
    "            pattern = re.compile(terminal)\n",
    "        \n",
    "            if pattern.search(grammar_cats):\n",
    "                self.grammar_cats[terminal].append((base, token))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polimorf = PolimorfGen()\n",
    "\n",
    "# Extract the categories\n",
    "for line in polimorf:\n",
    "    polimorf.find_terminal_occ(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some sentences of length n without using the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def core_sent_gen(n: int) -> str:\n",
    "    \n",
    "    # Draw the sentence schema\n",
    "    try:\n",
    "        schemas_n_len = schemas[n]\n",
    "    except KeyError:\n",
    "        print('Sentence length too large')  \n",
    "        return None\n",
    "        \n",
    "    schema = random.choice(schemas_n_len)\n",
    "    \n",
    "    # Draw the tokens\n",
    "    tokens_with_bases = [random.choice(polimorf.grammar_cats[category])\n",
    "                         for category in schema]\n",
    "    \n",
    "    bases, tokens = list(zip(*tokens_with_bases))\n",
    "    \n",
    "    return ' '.join(bases), ' '.join(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('policentrycznie rozpić aforystka', 'policentrycznie rozpito aforystki')"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_sent_gen(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('szczęśliwicko poprzypasywać przystępny nierozsiewczy kanterberyjka',\n",
       " 'szczęśliwicko poprzypasywano przystępne nierozsiewcze kanterberyjki')"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_sent_gen(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('siekierkowsko nieciągliwie ubruttowić szerokoekranowy przekształcający lampiarz',\n",
       " 'siekierkowsko nieciągliwie ubruttowiono szerokoekranowego przekształcającego lampiarza')"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_sent_gen(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence length too large\n"
     ]
    }
   ],
   "source": [
    "core_sent_gen(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Word2Vec struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusGen:\n",
    "    CORPUS_PATH = './data/task3_train_segmented.txt'\n",
    "    \n",
    "    def __init__(self, n_sent):\n",
    "        self.n_sent = n_sent\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(self.CORPUS_PATH) as f:\n",
    "            for line, _ in zip(f, range(self.n_sent)):\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = CorpusGen(10_000_000)\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "model.save('./data/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists('./data/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
